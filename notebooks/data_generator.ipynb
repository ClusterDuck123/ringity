{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from itertools import combinations\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.measure import compare_mse as mse\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import tarfile\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ringity as rng\n",
    "import urllib.request\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lipids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "subprocess.run(['executables/get_lipid_data.sh']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process\n",
    "usecols  = range(1,11)\n",
    "skiprows = [168]\n",
    "df = pd.read_excel('data/raw_data/lipids/1-s2.0-S0092867415006418-mmc3.xlsx', \n",
    "                   header    = 1, \n",
    "                   index_col = 0, \n",
    "                   usecols   = usecols, \n",
    "                   skiprows  = lambda x : x in skiprows)\n",
    "C = np.corrcoef(np.array(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "np.savetxt('data/biological_networks/lipids/lipid_corr.txt', C, fmt='%1.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genes \n",
    "<center><h3 style=\"color:darkred\"> >>> CAUTION - Big Data! <<<  </h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "subprocess.run(['executables/get_gene_data.sh']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip\n",
    "subprocess.run(['executables/unzip_gene_data.sh']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process\n",
    "df = pd.read_csv('data/raw_data/genes/circadiaNET_correlation_matrices/arabidopsis_thaliana_correlation_matrix.txt', \n",
    "                 header    = 0, \n",
    "                 index_col = 0, \n",
    "                 delimiter = ' ')\n",
    "C = np.corrcoef(np.array(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "np.savetxt('data/biological_networks/genes/gene_corr.txt', C, fmt='%1.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process\n",
    "name = 'FLS18 TNF'\n",
    "df = pd.read_excel('data/raw_data/cells/connectivity_FLS11_data.xls', sheet_name=name)\n",
    "D = squareform(pdist(df[['Position X','Position Y','Position Z']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "np.savetxt('data/biological_networks/cells/cell_D.txt',D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "subprocess.run(['executables/get_soil_data.sh']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract frames\n",
    "subprocess.run(['executables/extract_frames.sh']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This might take a while...    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(~4h on my computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process\n",
    "path = 'data/raw_data/soil/frames'\n",
    "pic_list = sorted(file for file in os.listdir(path) if file.endswith('jpg'))\n",
    "\n",
    "n = len(pic_list)\n",
    "\n",
    "D_mse  = np.zeros([n,n])\n",
    "C_ssim = np.ones([n,n])\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "for i, (jpg_a, jpg_b) in enumerate(combinations(pic_list, 2), 0):\n",
    "    \n",
    "    node_a = int(*re.findall(r'\\d+', jpg_a)) - 1\n",
    "    node_b = int(*re.findall(r'\\d+', jpg_b)) - 1\n",
    "    \n",
    "    if i%1000==0:\n",
    "        t2 = time.time()\n",
    "        print(f'{i/782.1:.3f}% - {t2-t1:.3f}sec')\n",
    "        \n",
    "    img_a = mpimg.imread(f'{path}/{jpg_a}',0)\n",
    "    img_b = mpimg.imread(f'{path}/{jpg_b}',0)\n",
    "\n",
    "    img_a = cv2.cvtColor(img_a, cv2.COLOR_BGR2GRAY)\n",
    "    img_b = cv2.cvtColor(img_b, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    D_mse[node_a, node_b] = mse( img_a, img_b)\n",
    "    D_mse[node_b, node_a] = D_mse[node_a, node_b]\n",
    "    \n",
    "    C_ssim[node_a, node_b] = ssim(img_a, img_b)\n",
    "    C_ssim[node_b, node_a] = C_ssim[node_a, node_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "np.savetxt('data/biological_networks/soil/soil_gray_mse.txt' , D_mse , fmt='%1.6f')\n",
    "np.savetxt('data/biological_networks/soil/soil_gray_ssim.txt', C_ssim, fmt='%1.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = \"homo_sapiens.sbml\"\n",
    "# If SMBL files are not yet in the working directory\n",
    "if not rootDir in os.listdir():\n",
    "    # Download all human reactions from Reactome in SBML format\n",
    "    urllib.request.urlretrieve(\"https://reactome.org/download/current/homo_sapiens.3.1.sbml.tgz\", \"/tmp/reactome_smbl.tgz\")\n",
    "    tar = tarfile.open(\"/tmp/reactome_smbl.tgz\")\n",
    "    tar.extractall(rootDir)\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathwayFile = 'R-HSA-913531.xml'\n",
    "\n",
    "sbml = ET.parse(rootDir + '/' + pathwayFile)\n",
    "model = sbml.getroot().find(\"{http://www.sbml.org/sbml/level3/version1/core}model\")\n",
    "reactions = model.find(\"{http://www.sbml.org/sbml/level3/version1/core}listOfReactions\")\n",
    "\n",
    "# List species annotated as \"simple chemical\" to remove them from networks, \n",
    "# to avoid creating star structures with the most common small molecules\n",
    "smallMolec = {term.attrib['id'] for term in model.find(\"{http://www.sbml.org/sbml/level3/version1/core}listOfSpecies\") \n",
    "  if ('sboTerm' in term.attrib.keys()) and (term.attrib['sboTerm'] == \"SBO:0000247\")} \n",
    "\n",
    "pathwayName = model.attrib['name']\n",
    "pathwayID = model.attrib['id']\n",
    "\n",
    "G = nx.DiGraph() \n",
    "\n",
    "# For each reaction in the pathway\n",
    "for reaction in reactions:\n",
    "    products = reaction.find(\"{http://www.sbml.org/sbml/level3/version1/core}listOfProducts\")\n",
    "    reagents = reaction.find(\"{http://www.sbml.org/sbml/level3/version1/core}listOfReactants\")\n",
    "    if not products or not reagents:\n",
    "#             print(\"No products or no reagents\")\n",
    "        break\n",
    "    products = {product.attrib['species'] for product in products if product.attrib['species'] not in smallMolec}\n",
    "    reagents = {reagent.attrib['species'] for reagent in reagents if reagent.attrib['species'] not in smallMolec}\n",
    "    # Add edge from reagents to products\n",
    "    G.add_edges_from([(r,p) for r in reagents for p in products])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(G, 'data/biological_networks/IFNs/IFN_edgelist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arctic GIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watts-Strogatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data with time stamps\n",
    "N = 100\n",
    "k = 6\n",
    "p = 1.0\n",
    "\n",
    "for i in range(7):\n",
    "    W = nx.watts_strogatz_graph(N,k,p)\n",
    "    dgm = rng.diagram(W, induce=True)\n",
    "    name = str(int(time.time()*10**6))+'.csv'\n",
    "    rng.save_dgm(dgm, f'data/toy_networks/watts_strogatz/dgms/N{N}/k{k}/p{p:.6f}/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect diagrams to a single score sheet\n",
    "N = 1024\n",
    "k = 8\n",
    "    \n",
    "path = f'data/toy_networks/watts_strogatz/dgms/N{N}/k{k}'\n",
    "p_list = sorted([float(file[1:]) for file in os.listdir(path) if file.startswith('p')])\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for p in p_list:\n",
    "    scores_tmp = []\n",
    "    for i, file in enumerate(os.listdir(f'{path}/p{p:.6f}')):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "        dgm = rng.load_dgm(fname=f'{path}/p{p:.6f}/{file}')\n",
    "        scores_tmp.append(dgm.GGS)\n",
    "\n",
    "    df_tmp = pd.DataFrame({p:scores_tmp})\n",
    "    df = pd.concat([df,df_tmp], axis=1)\n",
    "\n",
    "df.to_csv(f'data/toy_networks/watts_strogatz/GGS/N{N}/k{k}/GGS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erdos-Renyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**8\n",
    "    \n",
    "path = f'data/toy_networks/erdos_renyi/ER_annealing_10000/dgms/N{N}'\n",
    "p_list = sorted([float(file[1:]) for file in os.listdir(path) if file.startswith('p')])\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for p in p_list:\n",
    "    scores_tmp = []\n",
    "    for i, file in enumerate(os.listdir(f'{path}/p{p:.6f}')):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue         \n",
    "        dgm = rng.load_dgm(fname=f'{path}/p{p:.6f}/{file}')\n",
    "        scores_tmp.append(dgm.GGS)\n",
    "\n",
    "    df_tmp = pd.DataFrame({p:scores_tmp})\n",
    "    df = pd.concat([df,df_tmp], axis=1)\n",
    "\n",
    "df.to_csv(f'data/toy_networks/erdos_renyi/ER_annealing_10000/GGS/{N}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
